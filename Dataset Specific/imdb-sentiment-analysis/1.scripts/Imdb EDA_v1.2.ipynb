{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "2. Stop Word Removal\n",
    "3. Negation Handling\n",
    "4. Stemming/Lemmatization\n",
    "5. Classification\n",
    "6. Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    We saw this on the shelf at the local video st...\n",
       "1    Well, you'd better if you plan on sitting thro...\n",
       "2    This is my favorite Jackie Chan movie and in a...\n",
       "3    The long list of \"big\" names in this flick (in...\n",
       "4    The great and underrated Marion Davies shows h...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('..\\\\0.data\\\\raw\\\\imdb_train.csv')\n",
    "print(len(train))\n",
    "train.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lmtzr = stem.WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    stopList = stopwords.words('english')\n",
    "    \n",
    "    tokens = []\n",
    "    tokenList = word_tokenize(text)\n",
    "    for token in tokenList:\n",
    "        token = token.lower()\n",
    "        token = lmtzr.lemmatize(token)\n",
    "\n",
    "        tokens.append(token)\n",
    "\n",
    "    print(\"\\nBefore Removal\"); print(len(tokens))\n",
    "    tokensSR = [token for token in tokens if token not in stopList]\n",
    "    tokens = tokensSR\n",
    "    print(\"\\nAfter Removal\"); print(len(tokens))\n",
    "    \n",
    "    print(tokens)\n",
    "    tokenList.clear()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "'''\n",
    "Split the para.\n",
    "Get words.\n",
    "convert to lower case\n",
    "lemmatize them.\n",
    "join them back.\n",
    "Now the paragraph is clean.\n",
    "'''\n",
    "def clean_paragraph(para):\n",
    "# 77.86927056312561\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     para = para.translate(table)\n",
    "#     p = ' '.join([lmtzr.lemmatize(token.lower()) for token in para.split(' ')])    \n",
    "\n",
    "\n",
    "# 45.1863751411438\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    p = ' '.join([lmtzr.lemmatize(token.lower()) for token in tokenizer.tokenize(para)])\n",
    "\n",
    "\n",
    "# 42.43521022796631 But removes words like word. word! word, wo'rd\n",
    "#     p = ' '.join([lmtzr.lemmatize(token.lower()) for token in para.split(' ') if token.isalpha()])\n",
    "\n",
    "# 121.97609329223633\n",
    "#     p = ' '.join([lmtzr.lemmatize(token.lower()) for token in word_tokenize(para)])\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.81619048118591\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "train.text = [clean_paragraph(para) for para in train.text]\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  labels                                               text\n",
      "0   1288       0  we saw this on the shelf at the local video st...\n",
      "1   2064       0  well you d better if you plan on sitting throu...\n",
      "2  18997       1  this is my favorite jackie chan movie and in a...\n",
      "3  10448       0  the long list of big name in this flick includ...\n",
      "4  16133       1  the great and underrated marion davy show her ...\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "train.to_csv('..\\\\0.data\\\\processed\\\\imdb_train_cleaned.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
